{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 22:51:45.581551: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.datasets import mnist, fashion_mnist\n",
    "import tensorflow.keras as kb\n",
    "from tensorflow.keras import backend\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced CNN Techniques\n",
    "- Data Augmentation\n",
    "- Residual Connections\n",
    "- Depthwise Separable Connections\n",
    "- Visualizing Layer Activations\n",
    "- Visualizing Filters\n",
    "- Grad CAM\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Data Augmentation is a method specific to CNNs that allows us to regularize our Network by showing the network the same image over and over with *slight* changes to things like:\n",
    "\n",
    "- rotation\n",
    "- crop\n",
    "- zoom\n",
    "- translation\n",
    "\n",
    "This is also helpful because we want our CNN to be able to recognize images even when they're shifted a little bit. \n",
    "\n",
    "Keras has a bunch of [`Random_*` layers](https://keras.io/api/layers/preprocessing_layers/image_augmentation/) which perform data augmentation for us. We can add a stack of them to a CNN before the first layer in order to perform data augmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = kb.utils.to_categorical(y_train, 10)\n",
    "y_test = kb.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " random_flip (RandomFlip)    (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " random_zoom (RandomZoom)    (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " random_rotation (RandomRot  (None, 28, 28, 1)         0         \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               1254500   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1274326 (4.86 MB)\n",
      "Trainable params: 1274326 (4.86 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = kb.Sequential([\n",
    "    kb.layers.InputLayer(input_shape =(28,28,1)),\n",
    "    kb.layers.RandomFlip(),\n",
    "    kb.layers.RandomZoom(0.2),\n",
    "    kb.layers.RandomRotation(0.1),\n",
    "    kb.layers.Conv2D(32, (3, 3), activation='relu', padding = \"same\"),\n",
    "    kb.layers.Conv2D(64, (3, 3), activation='relu', padding = \"same\"),\n",
    "    kb.layers.MaxPooling2D((2, 2)),\n",
    "    kb.layers.Flatten(),\n",
    "    kb.layers.Dense(100, activation='relu'),\n",
    "    kb.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 127s 67ms/step - loss: 0.7319 - accuracy: 0.7250 - val_loss: 0.5496 - val_accuracy: 0.7964\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 109s 58ms/step - loss: 0.5397 - accuracy: 0.7992 - val_loss: 0.4894 - val_accuracy: 0.8257\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 101s 54ms/step - loss: 0.4826 - accuracy: 0.8209 - val_loss: 0.4590 - val_accuracy: 0.8356\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 96s 51ms/step - loss: 0.4474 - accuracy: 0.8352 - val_loss: 0.4609 - val_accuracy: 0.8317\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 96s 51ms/step - loss: 0.4241 - accuracy: 0.8429 - val_loss: 0.4492 - val_accuracy: 0.8362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb9d1cd2580>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, validation_data = (x_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Connections\n",
    "\n",
    "Residual Connections create two paths for data to flow through:\n",
    "\n",
    "1. the traditional path which pushes the input image through the convolutional + pooling layers\n",
    "2. a non-destructive path that passes the image through unchanged (though we may reduce the size to match the output of the other path)\n",
    "\n",
    "These two paths are then added together before passing the output through to the next block of conv + pool layers. \n",
    "\n",
    "Because we're no longer using a simple, sequential stack of layers, we have to use the Functional API to define this architechture rather than `Sequential()`. ðŸ˜­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 28, 28, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 32)           320       ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 26, 26, 64)           18496     ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 26, 26, 64)           2112      ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 26, 26, 64)           0         ['conv2d_3[0][0]',            \n",
      "                                                                     'conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 13, 13, 64)           0         ['add[0][0]']                 \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 10816)                0         ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 100)                  1081700   ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 10)                   1010      ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1103638 (4.21 MB)\n",
      "Trainable params: 1103638 (4.21 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = kb.Input(shape = (28,28,1))\n",
    "x = kb.layers.Conv2D(32, (3,3), activation = \"relu\")(input)\n",
    "residual = x\n",
    "x = kb.layers.Conv2D(64, (3,3), padding = \"same\", activation = \"relu\")(x)\n",
    "residual = kb.layers.Conv2D(64,(1,1))(residual)\n",
    "x = kb.layers.add([x,residual])\n",
    "x = kb.layers.MaxPooling2D((2, 2))(x)\n",
    "x = kb.layers.Flatten()(x)\n",
    "x = kb.layers.Dense(100, activation='relu')(x)\n",
    "output = kb.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = kb.Model(inputs = input, outputs = output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 89s 47ms/step - loss: 0.3506 - accuracy: 0.8751 - val_loss: 0.2650 - val_accuracy: 0.9007\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 88s 47ms/step - loss: 0.2191 - accuracy: 0.9192 - val_loss: 0.2417 - val_accuracy: 0.9119\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 86s 46ms/step - loss: 0.1653 - accuracy: 0.9377 - val_loss: 0.2498 - val_accuracy: 0.9122\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 86s 46ms/step - loss: 0.1226 - accuracy: 0.9546 - val_loss: 0.2349 - val_accuracy: 0.9209\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 85s 45ms/step - loss: 0.0891 - accuracy: 0.9673 - val_loss: 0.2852 - val_accuracy: 0.9195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb9d3a4feb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, validation_data = (x_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depthwise Separable Convolutions\n",
    "\n",
    "[Depthwise Separable Convolutions](https://keras.io/api/layers/convolution_layers/separable_convolution2d/) are a way to reduce the computational cost of performing a convolution. DSC's apply a filter to each channel in an input image *separately* and then convolve those outputs together. This does assume that the different channels in the input are highly independent, but that's often true so it typically won't reduce the performance of your model unless that assumption is violated.\n",
    "\n",
    "Luckily, `SeparableConv2D()` is a drop-in replacement for `Conv2D()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " separable_conv2d (Separabl  (None, 28, 28, 32)        73        \n",
      " eConv2D)                                                        \n",
      "                                                                 \n",
      " separable_conv2d_1 (Separa  (None, 28, 28, 64)        2400      \n",
      " bleConv2D)                                                      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 14, 14, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               1254500   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1257983 (4.80 MB)\n",
      "Trainable params: 1257983 (4.80 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = kb.Sequential([\n",
    "    kb.layers.InputLayer(input_shape =(28,28,1)),\n",
    "    kb.layers.SeparableConv2D(32, (3, 3), activation='relu', padding = \"same\"),\n",
    "    kb.layers.SeparableConv2D(64, (3, 3), activation='relu', padding = \"same\"),\n",
    "    kb.layers.MaxPooling2D((2, 2)),\n",
    "    kb.layers.Flatten(),\n",
    "    kb.layers.Dense(100, activation='relu'),\n",
    "    kb.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.4173 - accuracy: 0.8529 - val_loss: 0.3311 - val_accuracy: 0.8802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb9c2602460>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1, validation_data = (x_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Layer Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " c1 (Conv2D)                 (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " c2 (Conv2D)                 (None, 28, 28, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 14, 14, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                125450    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 144266 (563.54 KB)\n",
      "Trainable params: 144266 (563.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = kb.Sequential([\n",
    "    kb.layers.InputLayer(input_shape =(28,28,1)),\n",
    "    kb.layers.Conv2D(32, (3, 3), activation='relu', padding = \"same\", name = \"c1\"),\n",
    "    kb.layers.Conv2D(64, (3, 3), activation='relu', padding = \"same\", name = \"c2\"),\n",
    "    kb.layers.MaxPooling2D((2, 2)),\n",
    "    kb.layers.Flatten(),\n",
    "    kb.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 83s 44ms/step - loss: 0.3745 - accuracy: 0.8675 - val_loss: 0.2947 - val_accuracy: 0.8893\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 80s 43ms/step - loss: 0.2468 - accuracy: 0.9124 - val_loss: 0.2555 - val_accuracy: 0.9083\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 80s 42ms/step - loss: 0.2022 - accuracy: 0.9271 - val_loss: 0.2387 - val_accuracy: 0.9178\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 79s 42ms/step - loss: 0.1704 - accuracy: 0.9390 - val_loss: 0.2556 - val_accuracy: 0.9105\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 79s 42ms/step - loss: 0.1433 - accuracy: 0.9493 - val_loss: 0.2441 - val_accuracy: 0.9169\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 81s 43ms/step - loss: 0.1213 - accuracy: 0.9563 - val_loss: 0.2486 - val_accuracy: 0.9189\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 81s 43ms/step - loss: 0.1034 - accuracy: 0.9622 - val_loss: 0.2828 - val_accuracy: 0.9161\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 82s 44ms/step - loss: 0.0888 - accuracy: 0.9677 - val_loss: 0.2802 - val_accuracy: 0.9164\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 83s 45ms/step - loss: 0.0768 - accuracy: 0.9728 - val_loss: 0.3006 - val_accuracy: 0.9186\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 84s 45ms/step - loss: 0.0678 - accuracy: 0.9758 - val_loss: 0.3376 - val_accuracy: 0.9117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb9b1db7520>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " c1 (Conv2D)                 (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " c2 (Conv2D)                 (None, 28, 28, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 14, 14, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18816 (73.50 KB)\n",
      "Trainable params: 18816 (73.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layer_out = []\n",
    "layer_name = []\n",
    "\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, (kb.layers.Conv2D, kb.layers.MaxPooling2D)):\n",
    "        layer_out.append(layer.output)\n",
    "        layer_name.append(layer.name)\n",
    "\n",
    "layer_activations = kb.Model(inputs = model.input, outputs = layer_out)\n",
    "\n",
    "layer_activations.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 157ms/step\n"
     ]
    }
   ],
   "source": [
    "img = kb.utils.img_to_array(x_train[0])\n",
    "img = np.expand_dims(img, axis = 0)\n",
    "activations = layer_activations.predict(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb9a3f2dc70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAASO0lEQVR4nO3de4xc1X0H8O93Zl9+Yq8f6wcLNmYLgRBMs5gkoBaEQglSC0gVglaURqiOqqAGKVKLaNUgtYpQlEeJVCGZBOFEBERLEC6yEoiVlKKA64Ua29gYG7PGa3b9Wr9NvDszv/6xl2iBPb+7zJ2ZO+z5fiRrd+c3Z+b4er++M3PuOYdmBhGZ+gp5d0BEGkNhF4mEwi4SCYVdJBIKu0gkWhr5ZG1stw7MaORTikTldziNETvLiWqZwk7yRgAPASgC+JGZPejdvwMzcBWvz/KUIuLYaBuCtapfxpMsAvh3AF8BcAmAO0heUu3jiUh9ZXnPvgrAbjPbY2YjAJ4EcHNtuiUitZYl7EsB7Bv380By24eQXE2yj2TfKM5meDoRyaLun8ab2Roz6zWz3la01/vpRCQgS9j3A+ge9/O5yW0i0oSyhH0TgB6Sy0m2AbgdwLradEtEaq3qoTczK5G8B8AvMTb09qiZvVGznolITWUaZzez9QDW16gvIlJHulxWJBIKu0gkFHaRSCjsIpFQ2EUiobCLREJhF4mEwi4SCYVdJBIKu0gkFHaRSCjsIpFQ2EUi0dClpOXTpziv079D5xy3XN61p3adkUx0ZheJhMIuEgmFXSQSCrtIJBR2kUgo7CKRUNhFIqFx9sgVey5w63v/fJFbX/LS+/7jz740WNt/3Tlu26UP9bl1Gx1x6/JhOrOLREJhF4mEwi4SCYVdJBIKu0gkFHaRSCjsIpHQOHvkTly+wK0vfM0fyy6+vNWtH/7qlcFaud1tip0/XOnWOwb9X98LHgnPpS8NDvlPPgVlCjvJfgAnAZQBlMystxadEpHaq8WZ/TozO1yDxxGROtJ7dpFIZA27AXie5KskV090B5KrSfaR7BvF2YxPJyLVyvoy/hoz209yIYAXSL5pZi+Ov4OZrQGwBgBms9MyPp+IVCnTmd3M9idfDwJ4BsCqWnRKRGqv6rCTnEFy1gffA7gBwLZadUxEaivLy/guAM+Q/OBxfmZmv6hJr6RhWs5U3HrbL1PmlKc8fmE0XFv2Hwf8xx4YdOvH/+xzbv3Qj2YFa0d2LXPbLv2Nf1wKZ1P+5vTLxy5sDdYW/2bYbVvZ8qb/4AFVh93M9gC4vNr2ItJYGnoTiYTCLhIJhV0kEgq7SCQUdpFIaIrrFDf81S+69Vn76rsc88yB8OPv+csut237Ub++6N9+6z/5k+HSvM8V3aYDN/hbVVtKcs7/ab9bL7d3B2tpQ2ul6z8frNnGl4M1ndlFIqGwi0RCYReJhMIuEgmFXSQSCrtIJBR2kUhonH2KO97j1xes3+fWyymPX/yM/wQ8+rtgbfnTp922Z86b7dbf+bZ/DcFoZ7j3s9/0f/XP/dUxt17ZvN2tn7zFX8elOOJPofWU25xztDO1Vmd2kUgo7CKRUNhFIqGwi0RCYReJhMIuEgmFXSQSGmef4noe9sfRSwcOZnr88o5dVbctLPC3i55Ofz3mFT/zl3NmOTzOfuqiuW7bt/7KH+O3e8NzygGgvd+P1nkPpMzFd5xYFn7s8qbwMdOZXSQSCrtIJBR2kUgo7CKRUNhFIqGwi0RCYReJhMbZp7jR7nlunfsGMj1+yyJ/bffSUHhb5vKhQ/6Dp9UzmPamP4bf85y/rnxhVng7aAConDnj1ovd5wZrNuKv5d/iPDSdafKpZ3aSj5I8SHLbuNs6Sb5Aclfy1b9CQURyN5mX8Y8BuPEjt90HYIOZ9QDYkPwsIk0sNexm9iKA4Y/cfDOAtcn3awHcUttuiUitVfuevcvMBpPvhwAE37iRXA1gNQB0YHqVTyciWWX+NN7MDEBwRoKZrTGzXjPrbUV71qcTkSpVG/YDJBcDQPI129QpEam7asO+DsBdyfd3AXi2Nt0RkXpJfc9O8gkA1wKYT3IAwLcAPAjgKZJ3A9gL4LZ6dlJ83phvqej/f+6PNk9CypxztoffuhWmdfiPvXihW06bS9+y/PxwseKv227HT7h1zp7ptz961K2XnOsbihcud9ue6g4f83JbuF1q2M3sjkDp+rS2ItI8dLmsSCQUdpFIKOwikVDYRSKhsItEQlNcp4DR3vC2ye/Pb3Xb+gNIQPHSi/w7nPC3XS4uWRSsjZzX6bZt21O/Ka5pQ2s2WvLrs2bUsjsfUt79jluf+1Z4WvFAeIdsndlFYqGwi0RCYReJhMIuEgmFXSQSCrtIJBR2kUhonH0KaNvuTJfMuCVzpc3/FSlM96epji4MT78dme1fA1Ccf45fT1mu2Z3G2uVvF81B/7iVt/vTa8/cepVbn/7MRrfumfmfm4K1Qjl83YPO7CKRUNhFIqGwi0RCYReJhMIuEgmFXSQSCrtIJDTOPgXsu/PCYG3Jd7ONsxfeHXTrlZR54YWd4XnhKQtJo7g4PBceAGyuPw5vx8J9qwylHJdKcJMjAEBxhbNMNYC24/58eE/hsxf7d9i7P1jiqfD5W2d2kUgo7CKRUNhFIqGwi0RCYReJhMIuEgmFXSQSGmdvgJYLlrn1w1cvdut2+2G33rbOHxP2sNXZ4xeAnfbnjHtbMgNA4ZzZwVrlxCm3bfmQ//e2QX8su7ggPGe9cMF5blsMH/frKX1v/Z/wGgMAYKsuC9bOdE1z27bPCh9ze93ZItt9VAAkHyV5kOS2cbc9QHI/yc3Jn5vSHkdE8jWZl/GPAbhxgtt/YGYrkz/ra9stEam11LCb2YsAhhvQFxGpoywf0N1DckvyMn9u6E4kV5PsI9k3irMZnk5Esqg27A8DWAFgJYBBAN8L3dHM1phZr5n1tsL/MEdE6qeqsJvZATMrm1kFwCMAVtW2WyJSa1WFneT4saJbAWwL3VdEmkPqODvJJwBcC2A+yQEA3wJwLcmVAAxAP4Cv1aIzxXn+ft3lI/X7nLBw+Wfc+rFLwnOnj97q71F+cZc/d/rIJn+c/LyH/Hnbrc+/HKyljqN/3p87ba9scetpikvDc9Kt51y3LUvOuu8A0OefY8qHnP3dvRqAlmX+OPzZ5fPdetuQ/292dmb436Xjv/7XbXvob78YrJV2hc/fqWE3szsmuPnHae1EpLnoclmRSCjsIpFQ2EUiobCLREJhF4lEQ6e4Vjpn4OSffCFYH7yu7La/7OLwcMW6nl+4bV89O+LWnzvh1x/f0RuszVk/0237/mNb3foKHHDrhQ5/0WVvgMpG/b9Xy15/WHD0S5f77Y/6U2DLO8JbGxemT3fbFhYtdOvDfxH+XQKA4UsZrK344dtu21L/u269mFKvpEz9bS8tDdb8FAAnl4VrZWekVWd2kUgo7CKRUNhFIqGwi0RCYReJhMIuEgmFXSQSDR1nH50GHLoiPPZ50d/50ym9EeObum9129qgP55sI6NufUXHO8FaYV5wVS4AQKlQdOuo+COrha7wksgAUF4SnhrMl19325YGh9w6U+qjf3yFWy93h69PaH2+z21b2dPv1men1Z1a2lh2YcYMt842f+pw5ZQ/7bm8a09KD8JaToczROeiC53ZRSKhsItEQmEXiYTCLhIJhV0kEgq7SCQUdpFINHScvfU0sOiV8ECgna1+e6jy7vA4OJC+bXJpoTcqC5Rbwv8vlqf7h7Ftvv/Yhff9Mf607YO9sfR9//Qlt23HEX8Z6wUPh5epBoDCf/+fX3ervrR5/KUr/eW/2971t3z2VOb4axTwyAm3Xij7I/l0rs0oDfnrGzDtIoEAndlFIqGwi0RCYReJhMIuEgmFXSQSCrtIJBR2kUg0dJy9cOw0pj+zser2Led3B2ulvfvctqWUuc9ImV7s/a+Y9j+mP5INYIE/Xz3L9Qfd//pbt/7e3/vj8P+85zW3/i/9f+rWd769JFhrGfZ//bo3+NcftL+03a1XiuF1BMjwnHAAwKEj/mOntC/M97cfh7OPQdp6+h3D4d+oQsnpk98jgGQ3yV+T3E7yDZLfSG7vJPkCyV3JV38FBxHJ1WRexpcAfNPMLgHwBQBfJ3kJgPsAbDCzHgAbkp9FpEmlht3MBs3steT7kwB2AFgK4GYAa5O7rQVwS536KCI18Ines5NcBuAKABsBdJnZYFIaAtAVaLMawGoA6ID/XkRE6mfSn8aTnAngaQD3mtmHZgGYmSHwOZSZrTGzXjPrbYW/2Z2I1M+kwk6yFWNBf9zMfp7cfIDk4qS+GIC/fKuI5Cr1ZTzHxih+DGCHmX1/XGkdgLsAPJh8fbYuPRzHG15jq7+0L9ta3XrltL/0bxZs8Q8zZ/nLFlfO+Nsityw/P1izaf6rqSXf8Yfmvv3ULW7dFs1x63/wyia3noW3VXXe6vn7NPO98BzXwmh4WG4y79mvBnAngK0kNye33Y+xkD9F8m4AewHcNsm+ikgOUsNuZi8BCF1BcH1tuyMi9aLLZUUiobCLREJhF4mEwi4SCYVdJBINneKaFa+8LFgrDh1125b2DWR7bmesvHCOv1Q0Oue4ZWtNGYfv8MfKywODwZqNehtdpyv1v+vfIa1eR2nbKhdmhZeDtrI/Sl855i/fnfW4ZjH9vfeDtcKIM/21Hp0RkeajsItEQmEXiYTCLhIJhV0kEgq7SCQUdpFIfKrG2W3T1mCtlLK079C9/pLJ87b746atz/cFa+Ujw25bpNU/xYpzznHrdK5BsBnT3LbmbJM9GfbeoWCNzhg8AFR6wktgA0DL4VP+k6f8PpZ37vbbeyreNQIaZxeJnsIuEgmFXSQSCrtIJBR2kUgo7CKRUNhFIsGxzVwaYzY77Sp+Ohek9bY2HvWnVaM1ZQnxsr/kPdr8qdXhtX8BTD+YbXX12TtPuvXiYb9zWdcRqJu0LZsbmItPypvH/8qZ53C8fHjCv5zO7CKRUNhFIqGwi0RCYReJhMIuEgmFXSQSCrtIJCazP3s3gJ8A6MLYZNk1ZvYQyQcA/A2ADyYN329m6+vV0byl7WM+VaWNNpca0os6aOJx9DTe3u9m4esqJrN4RQnAN83sNZKzALxK8oWk9gMz++4n6aiI5GMy+7MPAhhMvj9JcgeApfXumIjU1id6z05yGYArAGxMbrqH5BaSj5KcG2izmmQfyb5RnM3WWxGp2qTDTnImgKcB3GtmJwA8DGAFgJUYO/N/b6J2ZrbGzHrNrLcV/p5lIlI/kwo7yVaMBf1xM/s5AJjZATMr29gnAo8AWFW/bopIVqlhJ0kAPwaww8y+P+72xePudiuAbbXvnojUymQ+jb8awJ0AtpLcnNx2P4A7SK7E2OhMP4Cv1aF/IlIjk/k0/iVMPGN6yo6pi0xFuoJOJBIKu0gkFHaRSCjsIpFQ2EUiobCLREJhF4mEwi4SCYVdJBIKu0gkFHaRSCjsIpFQ2EUiobCLRKKhWzaTPARg77ib5gM43LAOfDLN2rdm7RegvlWrln0738wWTFRoaNg/9uRkn5n15tYBR7P2rVn7Bahv1WpU3/QyXiQSCrtIJPIO+5qcn9/TrH1r1n4B6lu1GtK3XN+zi0jj5H1mF5EGUdhFIpFL2EneSHInyd0k78ujDyEk+0luJbmZZF/OfXmU5EGS28bd1knyBZK7kq8T7rGXU98eILk/OXabSd6UU9+6Sf6a5HaSb5D8RnJ7rsfO6VdDjlvD37OTLAJ4C8CXAQwA2ATgDjPb3tCOBJDsB9BrZrlfgEHyjwCcAvATM/tsctt3AAyb2YPJf5RzzewfmqRvDwA4lfc23sluRYvHbzMO4BYAf40cj53Tr9vQgOOWx5l9FYDdZrbHzEYAPAng5hz60fTM7EUAwx+5+WYAa5Pv12Lsl6XhAn1rCmY2aGavJd+fBPDBNuO5HjunXw2RR9iXAtg37ucBNNd+7wbgeZKvklydd2cm0GVmg8n3QwC68uzMBFK38W6kj2wz3jTHrprtz7PSB3Qfd42Z/SGArwD4evJytSnZ2HuwZho7ndQ23o0ywTbjv5fnsat2+/Os8gj7fgDd434+N7mtKZjZ/uTrQQDPoPm2oj7wwQ66ydeDOffn95ppG++JthlHExy7PLc/zyPsmwD0kFxOsg3A7QDW5dCPjyE5I/ngBCRnALgBzbcV9ToAdyXf3wXg2Rz78iHNso13aJtx5Hzsct/+3Mwa/gfATRj7RP5tAP+YRx8C/boAwOvJnzfy7huAJzD2sm4UY59t3A1gHoANAHYB+BWAzibq208BbAWwBWPBWpxT367B2Ev0LQA2J39uyvvYOf1qyHHT5bIikdAHdCKRUNhFIqGwi0RCYReJhMIuEgmFXSQSCrtIJP4fu+JuwoEjXVUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# which layer and channel you want to look at\n",
    "layer = 0\n",
    "channel = 3\n",
    "\n",
    "# grab layer and channel and plot\n",
    "layer_act = activations[layer]\n",
    "plt.imshow(layer_act[0,:,:,channel], cmap = \"viridis\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Filters\n",
    "\n",
    "We can also visualize the filters learned by your CNN by using *gradient **ascent*** to find an image that *maximally* activates the filter as it's slid across an input. \n",
    "\n",
    "We start with a blank image and then make iterative changes that maximize the filter output more and more at each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.get_layer(\"c1\")\n",
    "\n",
    "feature_extractor = kb.Model(inputs = model.input, outputs = layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(input_image, filter_index):\n",
    "    activation = feature_extractor(input_image)\n",
    "    # We avoid border artifacts by only involving non-border pixels in the loss.\n",
    "    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n",
    "    return tf.reduce_mean(filter_activation)\n",
    "\n",
    "@tf.function\n",
    "def gradient_ascent_step(img, filter_index, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(img)\n",
    "        loss = compute_loss(img, filter_index)\n",
    "    # Compute gradients.\n",
    "    grads = tape.gradient(loss, img)\n",
    "    # Normalize gradients.\n",
    "    grads = tf.math.l2_normalize(grads)\n",
    "    img += learning_rate * grads\n",
    "    return loss, img\n",
    "\n",
    "\n",
    "def initialize_image():\n",
    "    # We start from a gray image with some random noise\n",
    "    img = tf.random.uniform((1, 28, 28, 1))\n",
    "    # ResNet50V2 expects inputs in the range [-1, +1].\n",
    "    # Here we scale our random inputs to [-0.125, +0.125]\n",
    "    return (img - 0.5) * 0.25\n",
    "\n",
    "\n",
    "def visualize_filter(filter_index):\n",
    "    # We run gradient ascent for 20 steps\n",
    "    iterations = 30\n",
    "    learning_rate = 10.0\n",
    "    img = initialize_image()\n",
    "    for iteration in range(iterations):\n",
    "        loss, img = gradient_ascent_step(img, filter_index, learning_rate)\n",
    "\n",
    "    # Decode the resulting input image\n",
    "    img = deprocess_image(img[0].numpy())\n",
    "    return loss, img\n",
    "\n",
    "\n",
    "def deprocess_image(img):\n",
    "    # Normalize array: center on 0., ensure variance is 0.15\n",
    "    img -= img.mean()\n",
    "    img /= img.std() + 1e-5\n",
    "    img *= 0.15\n",
    "\n",
    "    # Center crop\n",
    "    img = img[2:-2, 2:-2, :]\n",
    "\n",
    "    # Clip to [0, 1]\n",
    "    img += 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    # Convert to RGB array\n",
    "    img *= 255\n",
    "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    "    return img\n",
    "    image -= image.mean()\n",
    "    image /= image.std()\n",
    "    image *= 64\n",
    "    image += 128\n",
    "    image = np.clip(image, 0, 255).astype(\"uint8\")\n",
    "    image = image[25:-25, 25:-25, :]\n",
    "    return(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAAAAADFHGIkAAACY0lEQVR4nAFYAqf9AajeDAPKS9DGLDbeHOTgSNAAAADfWt7p/wJFDa7B/kT+12Yb6AKm/1QmHQbky/0KCOYCyurKPPzAAB0SYPQfISG74isIB+kVJBP9BPNhAwmyaiPR0e1Y6N7k6SoQ2TkX761W3gLvyfjvcA2oHBFYAcckJAkxy9kWH6rSAhwCx7NMItJf4DAoM9oJ4uVLCJEMHBz+0OcqBEz//ZguK//TIBq/x0lD3/HlgCPL+BMOrAI2x3AfdRfuwOQ4/jQWj6oS99jKGhFU3a0E/+XuOw/3zwPh6RKhG2j/CjizDz0q+dn4AgoV4/LZlhZwNfbnGacxxAQc2jf2+uArOALJJB02tf1xAQCowBXGc+EyCTgtr6XhQhwC5zwbFthJqMY49B7zy01iy8T9ybE/agQWAvjj+gkz864Z6SX96FIq7oVXa/JSSo2lBgI35A8JBtpyMwMD6I3ooOQEYY3FMq7qVB0EHZLfwOIDZFo2Aw0sS2cA/eXWmuXgPRvFAgAB/iIxA93SUhkC5g5Uugj0OfpX4B/+AgIPCBvivffoHx7J+PMSBicYowIBNWrlAgAE8b0hCygN4A/4CREA9co76eMpFAYAywABAs7saAkCFLdS1ccgB9LiSCojwrjcIf7+AAIRVO7o76YgVKjWMO7xCOHeLEb9XO0VEQgCHA+tGc/cIxAKKUbwKCN790ENtieCFdfHBAPy8RcQOOXwyToOjzn/wMbk9uIV9H+x8wIC7Pr+KSK+xeI24NIu7CELkgr33Br0olMEGD4F8+9DLQMYDwg3T88l6QhgEiRl4itbs0kS2obEcZUAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "loss, img = visualize_filter(7)\n",
    "kb.preprocessing.image.save_img(\"0.png\", img)\n",
    "display(Image(\"0.png\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Class Activation Heatmaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Display\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb996dc4a30>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_builder = keras.applications.xception.Xception\n",
    "# img_size = (299, 299)\n",
    "# preprocess_input = keras.applications.xception.preprocess_input\n",
    "# decode_predictions = keras.applications.xception.decode_predictions\n",
    "\n",
    "img_size = (28, 28)\n",
    "last_conv_layer_name = \"max_pooling2d_3\"\n",
    "\n",
    "img = np.expand_dims(x_train[0], 0)\n",
    "img.shape\n",
    "\n",
    "plt.imshow(img.reshape((28,28)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_img_array(img, size):\n",
    "    # `array` is a float32 Numpy array of shape (28, 28, 1)\n",
    "    array = keras.preprocessing.image.img_to_array(img)\n",
    "    # We add a dimension to transform our array into a \"batch\"\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAECCAYAAAD+eGJTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAANj0lEQVR4nO3da4xc9XnH8d/Pu8suawy+lLoEuzVqERV1KESrBkiVRrGruITgvOgLUqggJN03bUOiSAhE1ajvqiZKE6lVopW5qXFJJMdpKMoF10mEqoCbxSDwBTCXFGwMdkwBx1bsXe/TFzOWzHbXC/M/8591n+9HsnZuzzzPjL0/n3PmzDmOCAHIa0GvBwDQW4QAkBwhACRHCADJEQJAcoQAkNy8CAHb62w/Y/s527dX7r3S9o9t77K90/atNfufMkef7cdtP9iD3ottb7L9tO3dtq+q3P9z7fd+h+37bQ91ud/dtg/Y3nHKbUttb7G9p/1zSeX+X2y//0/a/o7txd3qP13PQ8B2n6R/lvQnki6V9Anbl1YcYVLS5yPiUklXSvrLyv1PulXS7h70laSvSvpBRPyupN+vOYftCyV9RtJIRKyW1Cfp+i63vVfSumm33S5pa0RcLGlr+3rN/lskrY6IyyQ9K+mOLvZ/m56HgKQ/kPRcRLwQEcclfVPS+lrNI2J/RGxvXz6s1i/AhbX6S5LtFZI+KmlDzb7t3udJ+qCkuyQpIo5HxBuVx+iXdLbtfknDkl7pZrOIeFjS69NuXi/pvvbl+yR9vGb/iHgoIibbVx+VtKJb/aebDyFwoaSXT7m+V5V/CU+yvUrSFZK2VW79FUm3SZqq3FeSLpJ0UNI97dWRDbYX1moeEfskfUnSS5L2S3ozIh6q1f8UyyNif/vyq5KW92CGk26R9P1azeZDCMwLts+R9G1Jn42Ityr2vVbSgYh4rFbPafolvU/S1yLiCklH1N1F4bdpr3uvVyuM3iNpoe0ba/WfSbT2pe/J/vS271RrFXVjrZ7zIQT2SVp5yvUV7duqsT2gVgBsjIjNNXtL+oCk62z/XK1VoQ/b/kbF/nsl7Y2Ik0s/m9QKhVrWSnoxIg5GxISkzZKurtj/pNdsXyBJ7Z8Hag9g+2ZJ10q6ISp+qWc+hMDPJF1s+yLbZ6m1UeiBWs1tW6314d0R8eVafU+KiDsiYkVErFLrtf8oIqr9TxgRr0p62fYl7ZvWSNpVq79aqwFX2h5u/12sUW82kD4g6ab25Zskfbdmc9vr1FolvC4ijtbsrYjo+R9J16i1RfR5SXdW7v2Hai36PSnpifafa3r0PnxI0oM96Hu5pPH2e/BvkpZU7v93kp6WtEPSv0ga7HK/+9Xa/jCh1pLQpyQtU+tTgT2S/kPS0sr9n1Nr29jJf4Nfr/X+uz0UgKTmw+oAgB4iBIDkCAEgOUIASI4QAJKbVyFge5T+Oftnfu297j+vQkBST/8i6N/T/plfe0/7z7cQAFBZ1Z2FzvJgDGn2L6hN6JgGNFhtHvq/u/5e0L3/M47Hr3TWHMcSianufclyvr/3pX6lIzoexzzTff1d6zqDIS3U+72mZks0aME5i8qe4MSJovKpo3V3qf//ZFtsnfU+VgeA5AgBILmiEOjlAUIBNKPjEJgHBwgF0ICSJYGeHiAUQDNKQmDeHCAUQOe6/hFhe3fIUUka0nC32wF4l0qWBN7RAUIjYiwiRiJipJc7YwCYWUkI9PQAoQCa0fHqQERM2v4rST9U69RRd0fEzsYmA1BF0TaBiPiepO81NAuAHmCPQSA5QgBIruq3CHFmmzp8uNcjFOlb/utF9ScOHiobIMq+Cu2+vs6LJ2e/iyUBIDlCAEiOEACSIwSA5AgBIDlCAEiOEACSIwSA5AgBIDlCAEiOEACSIwSA5AgBIDlCAEiOEACS43gCSOPEawd6PUKRmDzNQQHmLJ79LpYEgOQIASA5QgBIjhAAkis5NflK2z+2vcv2Ttu3NjkYgDpKPh2YlPT5iNhue5Gkx2xviYhdDc0GoIKOlwQiYn9EbG9fPixptzg1OXDGaWSbgO1Vkq6QtK2J5wNQT/HOQrbPkfRtSZ+NiLdmuH9U0qgkDWm4tB2AhhUtCdgeUCsANkbE5pkeExFjETESESMDGixpB6ALSj4dsKS7JO2OiC83NxKAmkqWBD4g6c8lfdj2E+0/1zQ0F4BKOt4mEBH/KckNzgKgB9hjEEiOEACSIwSA5AgBIDlCAEiOEACSIwSA5AgBIDlCAEiOEACSIwSA5AgBIDlCAEiOEACSIwSA5AgBIDlCAEiOEACSIwSA5AgBIDlCAEiOEACSIwSA5IpDwHaf7cdtP9jEQADqamJJ4Fa1TksO4AxUekLSFZI+KmlDM+MAqK10SeArkm6TNFU+CoBeKDkr8bWSDkTEY3M8btT2uO3xCR3rtB2ALik9K/F1tn8u6ZtqnZ34G9MfFBFjETESESMDGixoB6AbOg6BiLgjIlZExCpJ10v6UUTc2NhkAKpgPwEguf4mniQifiLpJ008F4C6WBIAkiMEgOQIASA5QgBIjhAAkiMEgOQIASA5QgBIjhAAkiMEgOQIASA5QgBIjhAAkiMEgOQIASA5QgBIjhAAkiMEgOQIASA5QgBIjhAAkiMEgOQIASC50rMSL7a9yfbTtnfbvqqpwQDUUXryka9K+kFE/KntsyQNNzATgIo6DgHb50n6oKSbJSkijks63sxYAGopWR24SNJBSffYftz2BtsLG5oLQCUlIdAv6X2SvhYRV0g6Iun26Q+yPWp73Pb4hI4VtAPQDSUhsFfS3ojY1r6+Sa1QeJuIGIuIkYgYGdBgQTsA3dBxCETEq5Jetn1J+6Y1knY1MhWAako/HfhrSRvbnwy8IOmT5SMBqKkoBCLiCUkjzYwCoBfYYxBIjhAAkiMEgOQIASA5QgBIjhAAkiMEgOQIASA5QgBIjhAAkiMEgOQIASA5QgBIjhAAkiMEgOQIASA5QgBIjhAAkis9xiDOIOf/dHFR/cGr3yiq7/u9S+Z+0Gmc2PlMUf3rnyw7S9752w4V1fvNXxbVF/V+bWDW+1gSAJIjBIDkCAEgOUIASK4oBGx/zvZO2zts3297qKnBANTRcQjYvlDSZySNRMRqSX2Srm9qMAB1lK4O9Es623a/pGFJr5SPBKCmkhOS7pP0JUkvSdov6c2IeKipwQDUUbI6sETSekkXSXqPpIW2b5zhcaO2x22PT+hY55MC6IqS1YG1kl6MiIMRMSFps6Srpz8oIsYiYiQiRgY0WNAOQDeUhMBLkq60PWzbktZI2t3MWABqKdkmsE3SJknbJT3Vfq6xhuYCUEnRF4gi4guSvtDQLAB6gD0GgeQIASC5qscTmFqyUIc/cmXH9Yu+9WhR/0OfLvs++bINjxTVl7rt+aeK6v/ht99bVD+1dWVR/b4ty4rq9bH/8+HTu/K3t2wsqv+bzX9WVH/unrLXv+yuzv/9tT7AmxlLAkByhACQHCEAJEcIAMkRAkByhACQHCEAJEcIAMkRAkByhACQHCEAJEcIAMkRAkByhACQHCEAJOeIqNbsXC+N93tNx/ULhoeL+r+x/rKi+nNfPFpUr0efLCpfMFR2lreja8te/9CD/1VU/8K/Xl5UP/nLgaL63/x3F9UvfPb1onr1lf2fe2LXsx3Xbouteiten/ENYEkASI4QAJIjBIDkCAEguTlDwPbdtg/Y3nHKbUttb7G9p/1zSXfHBNAt72RJ4F5J66bddrukrRFxsaSt7esAzkBzhkBEPCxp+mcj6yXd1758n6SPNzsWgFo63SawPCL2ty+/Kml5Q/MAqKx4w2C09jaadY8j26O2x22PT+hYaTsADes0BF6zfYEktX8emO2BETEWESMRMTKgwQ7bAeiWTkPgAUk3tS/fJOm7zYwDoLZ38hHh/ZIekXSJ7b22PyXp7yX9se09kta2rwM4A815QtKI+MQsd3X+TSAA8wZ7DALJEQJAcnOuDswnU0fLvs9/6L1l3yefHFpYVP+LT48U1Q8/f1ZR/eD/lB074shfXFVUv+jhonIdWVlWv/+GI0X1MXVOUf2eD91bVL/2hls6L/7ZI7PexZIAkBwhACRHCADJEQJAcoQAkBwhACRHCADJEQJAcoQAkBwhACRHCADJEQJAcoQAkBwhACRHCADJVT2egIcG1bfqdzp/gv6+ov5nHyw7nsDSe35aVL/sqdVF9Z4o+z78gkNvFdVP7t1XVF/q/J52L/cRXV5U36ftnRfH7MfiYEkASI4QAJIjBIDkOj01+RdtP237Sdvfsb24q1MC6JpOT02+RdLqiLhM0rOS7mh4LgCVdHRq8oh4KCIm21cflbSiC7MBqKCJbQK3SPp+A88DoAeK9hOwfaekSUkbT/OYUUmjkjTUf25JOwBd0HEI2L5Z0rWS1kTErGe1iIgxSWOSdN7ZF5Sd/QJA4zoKAdvrJN0m6Y8iTrMrEoB5r9NTk/+TpEWStth+wvbXuzwngC7p9NTkd3VhFgA9wB6DQHKEAJAcIQAkV/V4Ajo+oXj5lY7LF5xXtp/Bb/zjM0X1pWJ8x9wPOl19Yf+pwnr0lvsLfl0nZ7+LJQEgOUIASI4QAJIjBIDkCAEgOUIASI4QAJIjBIDkCAEgOUIASI4QAJIjBIDkCAEgOUIASI4QAJLzaY4W3nwz+6Ck/z7NQ35N0i8qjUP/+dU/82uv0f+3IuL8me6oGgJzsT0eESP0z9c/82vvdX9WB4DkCAEgufkWAmP0T9s/82vvaf95tU0AQH3zbUkAQGWEAJAcIQAkRwgAyRECQHL/C4ti1b6N6RLhAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove last layer's softmax\n",
    "model.layers[-1].activation = None\n",
    "\n",
    "# Print what the top predicted class is\n",
    "preds = model.predict(img)\n",
    "\n",
    "# Generate class activation heatmap\n",
    "heatmap = make_gradcam_heatmap(img, model, last_conv_layer_name)\n",
    "\n",
    "# Display heatmap\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def save_and_display_gradcam(img, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :1]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    plt.matshow(superimposed_img.reshape((28,28)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "save_and_display_gradcam(img, heatmap, alpha = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occlusion\n",
    "\n",
    "I built this [Shiny App](https://cmparlett.shinyapps.io/convolutionalneuralnetocclusion/) for you to play around with. You can upload any image, and occlude (or cover up) part of it and see how that changes the classification of the image. My favorite example is that if you cover up a corgi butt, the model is less confident that the image is of a Corgi!\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1WPbTlOHlHY8T1XLQJUMHaQLsdwpm0dUE\" alt=\"Q\" width = â€œ400â€/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
