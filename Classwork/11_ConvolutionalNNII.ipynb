{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.datasets import mnist, fashion_mnist\n",
    "import tensorflow.keras as kb\n",
    "from tensorflow.keras import backend\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced CNN Techniques\n",
    "- Data Augmentation\n",
    "- Residual Connections\n",
    "- Depthwise Separable Connections\n",
    "- Visualizing Layer Activations\n",
    "- Visualizing Filters\n",
    "- Grad CAM\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Data Augmentation is a method specific to CNNs that allows us to regularize our Network by showing the network the same image over and over with *slight* changes to things like:\n",
    "\n",
    "- rotation\n",
    "- crop\n",
    "- zoom\n",
    "- translation\n",
    "\n",
    "This is also helpful because we want our CNN to be able to recognize images even when they're shifted a little bit. \n",
    "\n",
    "Keras has a bunch of [`Random_*` layers](https://keras.io/api/layers/preprocessing_layers/image_augmentation/) which perform data augmentation for us. We can add a stack of them to a CNN before the first layer in order to perform data augmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = kb.utils.to_categorical(y_train, 10)\n",
    "y_test = kb.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 19:00:45.919551: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " random_flip (RandomFlip)    (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " random_zoom (RandomZoom)    (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " random_rotation (RandomRota  (None, 28, 28, 1)        0         \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               1254500   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,274,326\n",
      "Trainable params: 1,274,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = kb.Sequential([\n",
    "    kb.layers.InputLayer(input_shape =(28,28,1)),\n",
    "    kb.layers.RandomFlip(),\n",
    "    kb.layers.RandomZoom(0.2),\n",
    "    kb.layers.RandomRotation(0.1),\n",
    "    kb.layers.Conv2D(32, (3, 3), activation='relu', padding = \"same\"),\n",
    "    kb.layers.Conv2D(64, (3, 3), activation='relu', padding = \"same\"),\n",
    "    kb.layers.MaxPooling2D((2, 2)),\n",
    "    kb.layers.Flatten(),\n",
    "    kb.layers.Dense(100, activation='relu'),\n",
    "    kb.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 71s 37ms/step - loss: 0.7326 - accuracy: 0.7290 - val_loss: 0.5581 - val_accuracy: 0.7942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed5ab4b520>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1, validation_data = (x_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Connections\n",
    "\n",
    "Residual Connections create two paths for data to flow through:\n",
    "\n",
    "1. the traditional path which pushes the input image through the convolutional + pooling layers\n",
    "2. a non-destructive path that passes the image through unchanged (though we may reduce the size to match the output of the other path)\n",
    "\n",
    "These two paths are then added together before passing the output through to the next block of conv + pool layers. \n",
    "\n",
    "Because we're no longer using a simple, sequential stack of layers, we have to use the Functional API to define this architechture rather than `Sequential()`. ðŸ˜­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 26, 26, 32)   320         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 26, 26, 64)   18496       ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 26, 26, 64)   2112        ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 26, 26, 64)   0           ['conv2d_3[0][0]',               \n",
      "                                                                  'conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 13, 13, 64)  0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 10816)        0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 100)          1081700     ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 10)           1010        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,103,638\n",
      "Trainable params: 1,103,638\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = kb.Input(shape = (28,28,1))\n",
    "x = kb.layers.Conv2D(32, (3,3), activation = \"relu\")(input)\n",
    "residual = x\n",
    "x = kb.layers.Conv2D(64, (3,3), padding = \"same\", activation = \"relu\")(x)\n",
    "residual = kb.layers.Conv2D(64,(1,1))(residual)\n",
    "x = kb.layers.add([x,residual])\n",
    "x = kb.layers.MaxPooling2D((2, 2))(x)\n",
    "x = kb.layers.Flatten()(x)\n",
    "x = kb.layers.Dense(100, activation='relu')(x)\n",
    "output = kb.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = kb.Model(inputs = input, outputs = output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 66s 35ms/step - loss: 0.3487 - accuracy: 0.8759 - val_loss: 0.2613 - val_accuracy: 0.9020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed49d15eb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1, validation_data = (x_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depthwise Separable Convolutions\n",
    "\n",
    "[Depthwise Separable Convolutions](https://keras.io/api/layers/convolution_layers/separable_convolution2d/) are a way to reduce the computational cost of performing a convolution. DSC's apply a filter to each channel in an input image *separately* and then convolve those outputs together. This does assume that the different channels in the input are highly independent, but that's often true so it typically won't reduce the performance of your model unless that assumption is violated.\n",
    "\n",
    "Luckily, `SeparableConv2D()` is a drop-in replacement for `Conv2D()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " separable_conv2d (Separable  (None, 28, 28, 32)       73        \n",
      " Conv2D)                                                         \n",
      "                                                                 \n",
      " separable_conv2d_1 (Separab  (None, 28, 28, 64)       2400      \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               1254500   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,257,983\n",
      "Trainable params: 1,257,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = kb.Sequential([\n",
    "    kb.layers.InputLayer(input_shape =(28,28,1)),\n",
    "    kb.layers.SeparableConv2D(32, (3, 3), activation='relu', padding = \"same\"),\n",
    "    kb.layers.SeparableConv2D(64, (3, 3), activation='relu', padding = \"same\"),\n",
    "    kb.layers.MaxPooling2D((2, 2)),\n",
    "    kb.layers.Flatten(),\n",
    "    kb.layers.Dense(100, activation='relu'),\n",
    "    kb.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 52s 27ms/step - loss: 0.4368 - accuracy: 0.8426 - val_loss: 0.3605 - val_accuracy: 0.8738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed3a0e43d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1, validation_data = (x_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Layer Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = kb.Sequential([\n",
    "#     kb.layers.InputLayer(input_shape =(28,28,1)),\n",
    "#     kb.layers.Conv2D(32, (3, 3), activation='relu', padding = \"same\"),\n",
    "#     kb.layers.Conv2D(64, (3, 3), activation='relu', padding = \"same\"),\n",
    "#     kb.layers.MaxPooling2D((2, 2)),\n",
    "#     kb.layers.Flatten(),\n",
    "#     kb.layers.Dense(10, activation='softmax'),\n",
    "# ])\n",
    "\n",
    "# model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# model.fit(x_train, y_train, epochs=25, validation_data = (x_test, y_test))\n",
    "# model.save(\"11_smallConvNet\")\n",
    "\n",
    "model = kb.models.load_model(\"11_smallConvNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,816\n",
      "Trainable params: 18,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layer_out = []\n",
    "layer_name = []\n",
    "\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, (kb.layers.Conv2D, kb.layers.MaxPooling2D)):\n",
    "        layer_out.append(layer.output)\n",
    "        layer_name.append(layer.name)\n",
    "\n",
    "layer_activations = kb.Model(inputs = model.input, outputs = layer_out)\n",
    "\n",
    "layer_activations.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 65ms/step\n"
     ]
    }
   ],
   "source": [
    "img = kb.utils.img_to_array(x_train[0])\n",
    "img = np.expand_dims(img, axis = 0)\n",
    "activations = layer_activations.predict(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fed6b1c1370>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# which layer and channel you want to look at\n",
    "layer = 0\n",
    "channel = 3\n",
    "\n",
    "# grab layer and channel and plot\n",
    "layer_act = activations[layer]\n",
    "plt.imshow(layer_act[0,:,:,channel], cmap = \"Greys\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Filters\n",
    "\n",
    "We can also visualize the filters learned by your CNN by using *gradient **ascent*** to find an image that *maximally* activates the filter as it's slid across an input. \n",
    "\n",
    "We start with a blank image and then make iterative changes that maximize the filter output more and more at each step.\n",
    "\n",
    "\n",
    "First, let's pull the names of the Convolutional Layers in our Model so we know what they're called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conv2d_4', 'conv2d_5']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_name2 = []\n",
    "\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, (kb.layers.Conv2D)):\n",
    "        layer_name2.append(layer.name)\n",
    "\n",
    "layer_name2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll pull the layer from the model and create a NEW model that returns the output of that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = \"conv2d_5\"\n",
    "\n",
    "layer = model.get_layer(layer_name)\n",
    "\n",
    "feature_extractor = kb.Model(inputs = model.input, outputs = layer.output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll define some functions to do the gradient ascent (modified from Chapter 9 in your book), but adapt it to work with a 1D image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(input_image, filter_index):\n",
    "    activation = feature_extractor(input_image)\n",
    "    # We avoid border artifacts by only involving non-border pixels in the loss.\n",
    "    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n",
    "    return tf.reduce_mean(filter_activation)\n",
    "\n",
    "@tf.function\n",
    "def gradient_ascent_step(img, filter_index, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(img)\n",
    "        loss = compute_loss(img, filter_index)\n",
    "    # Compute gradients.\n",
    "    grads = tape.gradient(loss, img)\n",
    "    # Normalize gradients.\n",
    "    grads = tf.math.l2_normalize(grads)\n",
    "    img += learning_rate * grads\n",
    "    return loss, img\n",
    "\n",
    "\n",
    "def initialize_image():\n",
    "    # We start from a gray image with some random noise\n",
    "    img = tf.random.uniform((1, 28, 28, 1))\n",
    "    # ResNet50V2 expects inputs in the range [-1, +1].\n",
    "    # Here we scale our random inputs to [-0.125, +0.125]\n",
    "    return (img - 0.5) * 0.25\n",
    "\n",
    "\n",
    "def visualize_filter(filter_index):\n",
    "    # We run gradient ascent for 20 steps\n",
    "    iterations = 30\n",
    "    learning_rate = 10.0\n",
    "    img = initialize_image()\n",
    "    for iteration in range(iterations):\n",
    "        loss, img = gradient_ascent_step(img, filter_index, learning_rate)\n",
    "\n",
    "    # Decode the resulting input image\n",
    "    img = deprocess_image(img[0].numpy())\n",
    "    return loss, img\n",
    "\n",
    "\n",
    "def deprocess_image(img):\n",
    "    # Normalize array: center on 0., ensure variance is 0.15\n",
    "    img -= img.mean()\n",
    "    img /= img.std() + 1e-5\n",
    "    img *= 0.15\n",
    "\n",
    "    # Center crop\n",
    "    img = img[2:-2, 2:-2, :]\n",
    "\n",
    "    # Clip to [0, 1]\n",
    "    img += 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    # Convert to RGB array\n",
    "    img *= 255\n",
    "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    "    return img\n",
    "    image -= image.mean()\n",
    "    image /= image.std()\n",
    "    image *= 64\n",
    "    image += 128\n",
    "    image = np.clip(image, 0, 255).astype(\"uint8\")\n",
    "    image = image[25:-25, 25:-25, :]\n",
    "    return(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use those functions to visualze the filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fed3a1ff2e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "filter_no = 20\n",
    "loss, img = visualize_filter(filter_no)\n",
    "\n",
    "plt.imshow(img.reshape((24,24)), cmap = \"Greys\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Class Activation Heatmaps\n",
    "\n",
    "Class activation heatmaps (CAM) are really helpful in showing us what in an image is causing the model to make it's classification. \n",
    "\n",
    "We do this by combining two things:\n",
    "\n",
    "- how much our image activates the various features learned by our last convolutional layer\n",
    "- how important those features are to classifying an image as a certain category\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fed6b2784c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_size = (28, 28)\n",
    "last_conv_layer_name = \"conv2d_5\"\n",
    "\n",
    "img = np.expand_dims(x_train[78], 0)\n",
    "img.shape\n",
    "\n",
    "plt.imshow(img.reshape((28,28)), cmap = \"Greys\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_img_array(img, size):\n",
    "    # `array` is a float32 Numpy array of shape (28, 28, 1)\n",
    "    array = kb.preprocessing.image.img_to_array(img)\n",
    "    # We add a dimension to transform our array into a \"batch\"\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = kb.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output,\n",
    "         model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove last layer's softmax\n",
    "model.layers[-1].activation = None\n",
    "\n",
    "# Print what the top predicted class is\n",
    "preds = model.predict(img)\n",
    "\n",
    "# Generate class activation heatmap\n",
    "heatmap = make_gradcam_heatmap(img, model, last_conv_layer_name)\n",
    "\n",
    "# Display heatmap\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling After Convolution\n",
    "\n",
    "In the lecture I *briefly* mentioned that for image segmentation, we want to produce a mask that is the *same size* as our input image, because each pixel of the mask tells us what category each pixel in the image should be. \n",
    "\n",
    "However we've been learning that CNNs typically reduce the size of the input image (or *downsample*) one or more times with Pooling layers. So if we want to get our input to its original shape, we need to *upsample* the image.\n",
    "\n",
    "We can do this with `Conv2DTranspose()` layers (learn more [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose)). \n",
    "\n",
    "\n",
    "`Conv2DTranspose()` layers *essentially* learn to undo `Conv2D()` layers. For example, if you have a `(100,100,64)` input and put it through a `Conv2D(128, (3,3), padding = \"same\", stride = 2)` layer, your output will be `(50,50,128)`. To make that `(100,100,64)` again we'd need to put it through a `Conv2DTranspose(64, (3,3), padding = \"same\", stride = 2)` layer. \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1jPHABc6LNHIBUFppsUXfB-Lm6TtH4AF-\" alt=\"Q\" width = \"400\"/>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Lbs1XzStTqkCOVxKK_FsWQzAaPZqktGV\" alt=\"Q\" width = \"400\"/>\n",
    "\n",
    "\n",
    "\n",
    "(Images from: https://stackoverflow.com/questions/68976745/in-keras-what-is-the-difference-between-conv2dtranspose-and-conv2d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from your book Deep Learning with Python by Chollet\n",
    "def get_model(img_size, num_classes):\n",
    "    inputs = keras.Input(shape=img_size + (3,))\n",
    "    x = kb.layers.Rescaling(1./255)(inputs)\n",
    "\n",
    "    x = kb.layers.Conv2D(64, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = kb.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = kb.layers.Conv2D(128, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = kb.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = kb.layers.Conv2D(256, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = kb.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "    x = kb.layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = kb.layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = kb.layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = kb.layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = kb.layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = kb.layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "\n",
    "    outputs = kb.layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "    model = kb..Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Class\n",
    "\n",
    "Read Through and play around with [this Grad-CAM example](https://keras.io/examples/vision/grad_cam/) from Keras' website (this is the code used to generate the Grad CAM images in your book). You can click \"View in Colab\" to play with it in Google Cola. These look a LOT better than the ones we just made because the images are larger and full color images instead of gray-scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
